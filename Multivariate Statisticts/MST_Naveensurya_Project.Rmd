---
title: "Yeast Data Set Multivariate Analysis"
author: "NAVEENSURYA V"
date: "6 March 2024"
output: pdf_document
---

**Loading data set** 

I choose a data set Yeast from UCL ML respiratory. I am loading csv file from the website directly
it represents the classification of different types of yeast based on its different components namely "V2","V3",...,"V9".


```{r}
yeast <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data")

head(yeast)
```

as we see here it contains 1484 rows with 10 columns. first column indicates name of particular yeast.

V1 represents Index. This is a unique identifier for each data point (gene) in the dataset.

V2 to V8 represents Mitochr, ER, Golgi, Vacuole, Cytosol, Nucleus, Lysosome, Peroxisome:
These columns represent different cellular compartments (organelles) within the yeast cell. Each column contains a binary value (0 or 1), indicating whether the corresponding protein encoded by the gene is localized in that specific compartment.

last column indicates groups like "MIT" , "CYT", etc...

**Meaning of each groups**


*1.MIT*

 Score of discriminant analysis of the amino acid content of the N-terminal region (20 residues long) of mitochondrial and non-mitochondrial proteins
 
*2.MCH*

 McGeoch's method for signal sequence recognition.
 	
*3.GVH* 

 von Heijne's method for signal sequence recognition.
 
*4.ALM*

Score of the ALOM membrane spanning region prediction program

*5.ERL*

 Presence of HDEL substring (thought to act as a signal for retention in the endoplasmic reticulum lumen). Binary attribute.
 
*6.POX*

 	Peroxisomal targeting signal in the C-terminus.
 	
*7.VAC* 

 Score of discriminant analysis of the amino acid content of vacuolar and extracellular proteins.
 

*8.NUC* 

Score of discriminant analysis of nuclear localization signals of nuclear and non-nuclear proteins.




```{r}
library(car)

scatterplotMatrix(yeast[c(2,3,4,5,8,9)])

```



Here,In this matrix scatterplot, the diagonal cells show histograms of each of the variables, in this case the concentrations of each variables (V2, V3, ..., V9)
here we can see all V2,..,V9 are concentrated on particular region. it is nor widely concentrated

Each non diagonal element in scatterplot refers plot between corresponding Vi and Vj where i,j is its position.




**Observation from the Scatterplot**

 1. V2 and V3 are positively correlated.
 
 2. V9 has highly positive kurtosis.
 
 3. V5,V8 and V9 has almost equal kurtosis.
 
 4. V4, V5 and V9 have skewness towards RIGHT.
 
 5. V8 have skewness towards LEFT.





```{r}
plot(yeast$V2, yeast$V3)
```
here, we plotted V2 vs. v3 , if we can see it properly we can infer it is positively correlated as we see in scatter plot. remaining graphs we didn't see any direct relationship by looking into the scatter plot. In the same way we can plot each plots and check result separately which we identified from the scatter plot




```{r}
plot(yeast$V3, yeast$V4)
```
here we can see, both are almost spreaded and we can conclude about correlation here.


**Profile plot**


```{r}
makeProfilePlot <- function(mylist,names)
  {
     require(RColorBrewer)
     # find out how many variables we want to include
     numvariables <- length(mylist)
     # choose 'numvariables' random colours
     colours <- brewer.pal(numvariables,"Set1")
     # find out the minimum and maximum values of the variables:
     mymin <- 1e+20
     mymax <- 1e-20
     for (i in 1:numvariables)
     {
        vectori <- mylist[[i]]
        mini <- min(vectori)
        maxi <- max(vectori)
        if (mini < mymin) { mymin <- mini }
        if (maxi > mymax) { mymax <- maxi }
     }
     # plot the variables
     for (i in 1:numvariables)
     {
        vectori <- mylist[[i]]
        namei <- names[i]
        colouri <- colours[i]
        if (i == 1) { plot(vectori,col=colouri,type="l",ylim=c(mymin,mymax)) }
        else         { points(vectori, col=colouri,type="l")                                     }
        lastxval <- length(vectori)
        lastyval <- vectori[length(vectori)]
        text((lastxval-10),(lastyval),namei,col="black",cex=0.6)
     }
  }

library(RColorBrewer)
names <- c("V2", "V3", "V4", "V5" ,"V8", "V9")
mylist <- list(yeast$V2,yeast$V3,yeast$V4,yeast$V5,yeast$V8,yeast$V9)
makeProfilePlot(mylist,names)

```


this profile plotting indicates all mean lies almost close to each other. Since it is densed we cant infer anything about standard deviation . 

So, we can plot less in profile plot to see the hidden pattern


```{r}
names23 <- c("V4", "V5")
mylist23 <- list(yeast$V4,yeast$V5)
makeProfilePlot(mylist23,names23)

```
**Observation from profile plot V4 vs V5**

*Differences in Means:*

The difference in means between V4 and V5 suggests that, on average, the values of V5 are higher or lower than those of V4. This difference in means could indicate a shift in the central tendency of the data between the two variables.


*Variability:*

The higher deviation of V5 compared to V4 implies that the data points of V5 are more spread out from the mean compared to V4. This could indicate greater variability or dispersion within the dataset of V5.


*Skewness:*

The higher deviation of V5 might also suggest that its distribution is more skewed compared to V4. If the distribution of V5 is more spread out towards one tail compared to the other, it could result in a higher standard deviation.


```{r}
names3 <- c("V7" ,"V8", "V9")
mylist3 <- list(yeast$V7,yeast$V8,yeast$V9)
makeProfilePlot(mylist3,names3)

```

**Observation from profile plot V4 vs V5**


*V7 Nearly Zero Everywhere with Few Red Areas:*

The predominance of zero values in V7, except for a few areas highlighted in red, suggests that V7 is mostly close to zero but occasionally takes non-zero values in certain regions. These non-zero values, represented in red, could indicate specific instances or conditions where V7 deviates from its usual pattern of being close to zero.


*Differences in Means between V8 and V9:*

The presence of different means between V8 and V9 implies that, on average, the values of V9 are higher or lower than those of V8. This difference in means could indicate a shift in the central tendency of the data between the two variables.


*Variability and Deviation:*

The higher deviation of V9 compared to V8, indicated by the green color (V9) being highly deviated compared to the blue color (V8), suggests that the data points of V9 are more spread out from the mean compared to V8. This higher variability could indicate a wider range of values or greater dispersion within the dataset of V9.




**Calculating Summary Statistics for Multivariate Data**

```{r}
sapply(yeast[2:9],mean)
```
Here we can see mean values for each of following columns V2,V3,V4,V6 and V8 have mean very close to each other as we see from profile plot.

**Observation of means**

*Consistency:* The means of V2, V3, and V4 are close to each other, around 0.5, indicating that the data represented by these variables have similar average values. This suggests consistency or similarity in those datasets.

*Outlier:* V7 has a mean of 0.4998854, which is very close to 0.5. This might indicate that the dataset represented by V7 has values very close to 0.5, suggesting potential clustering or a dominant value around this mean.

*Skewness:* The mean of V5 is noticeably lower than the others, at 0.261186. This might suggest that the data represented by V5 is skewed towards lower values, as the mean is pulled towards the lower end of the distribution.




```{r}
sapply(yeast[2:9],sd)
```

Here we can see Standard Deviation of each columns

**Observation of Standard deviations**


*Consistency:* If the standard deviations of the values are similar, it suggests that the datasets have similar levels of variability. Conversely, if there are large disparities in the standard deviations, it indicates that some datasets have more variability than others.

*Outliers:* A very large standard deviation relative to the mean might suggest the presence of outliers or extreme values in the dataset. These outliers can significantly impact the standard deviation, pulling it away from the mean.


```{r}
library(corrplot) 
library(gplots) # Load the gplots library for heatmap function

# Compute correlation matrix
correlation_matrix <- cor(yeast[2:9])
correlation_matrix
```
it gives correlation matrix for each data. 

**Observation from Correlation Matrix**

*Strength of Correlation:*

1. Variables with correlation coefficients close to 1 or -1 have a strong linear relationship. For example, V2 and V3 have a correlation coefficient of 0.5816, indicating a moderately positive correlation.

2. Variables with correlation coefficients close to 0 suggest a weak or no linear relationship. For example, the correlation coefficient between V6 and V7 is very close to 0, indicating a weak linear relationship between these variables.


*Direction of Correlation:*

1. Positive correlation coefficients (closer to 1) indicate that when one variable increases, the other variable tends to increase as well. For example, V2 and V5 have a positive correlation coefficient of 0.1582.

2. Negative correlation coefficients (closer to -1) indicate that when one variable increases, the other variable tends to decrease. For example, V2 and V4 have a negative correlation coefficient of -0.1639.




we can plot using ggcorrplot to get analyse



```{r}
library(ggcorrplot)
ggcorrplot(correlation_matrix,lab = T,type = "upper",digits = 3,title = "Correlation Matrix ")
```
as we see above, V2 and V3 are highly correlated and we can get top 10 highly correlated by using this code.
We can see the correlation matrix above,
 1. we can clearly see V2 is highly positively correlated with V2 as we saw from scatter plot



```{r}
mosthighlycorrelated <- function(mydataframe, numtoreport) {
  # find the correlations
  cormatrix <- cor(mydataframe)
  
  # set the correlations in the upper triangle and diagonal to zero,
  # so they will not be reported as the highest ones:
  cormatrix[upper.tri(cormatrix, diag = TRUE)] <- 0
  
  # flatten the matrix into a dataframe for easy sorting
  indices <- which(cormatrix != 0, arr.ind = TRUE)
  fm <- data.frame(First.Variable = rownames(cormatrix)[indices[, 1]],
                   Second.Variable = colnames(cormatrix)[indices[, 2]],
                   Correlation = cormatrix[indices])
  
  # sort and print the top n correlations
  fm <- fm[order(abs(fm$Correlation), decreasing = TRUE), ]
  head(fm, n = numtoreport)
}


```

```{r}
mosthighlycorrelated(yeast[2:8], 10)
```
here we see most highly correlated values. as we see from correlation ploting V2, V3 are highly correlated.



**Mean and Standard Deviations by each Group**


According to "V10" column we get there are multiple groups. we can calculate the mean and standard deviations within each groups

```{r}
printMeanAndSdByGroup <- function(variables,groupvariable)
  {
     # find the names of the variables
     variablenames <- c(names(groupvariable),names(as.data.frame(variables)))
     # within each group, find the mean of each variable
     groupvariable <- groupvariable[,1] # ensures groupvariable is not a list
     means <- aggregate(as.matrix(variables) ~ groupvariable, FUN = mean)
     names(means) <- variablenames
     print(paste("Means:"))
     print(means)
     # within each group, find the standard deviation of each variable:
     sds <- aggregate(as.matrix(variables) ~ groupvariable, FUN = sd)
     names(sds) <- variablenames
     print(paste("Standard deviations:"))
     print(sds)
     # within each group, find the number of samples:
     samplesizes <- aggregate(as.matrix(variables) ~ groupvariable, FUN = length)
     names(samplesizes) <- variablenames
     print(paste("Sample sizes:"))
     print(samplesizes)
}

printMeanAndSdByGroup(yeast[2:8],yeast[10])
```
From, this we can get group-wise Mean , standard Deviation.

**Observation from group-wise mean**


*V2-V5:* Show variability in mean values across groups. V2 and V3 have higher means in the ERL group, while V4 has the highest mean in the CYT group. V5 varies, with the highest mean in the MIT group.


*V6-V7:* Consistently close to zero across groups, indicating low variability and little influence on group differences.


*V8:* Shows variability in mean values across groups. Highest mean in ERL, lowest in EXC.

**Observations from group-wise Standard deviation**

*V2-V5:*

Standard deviation values for V2-V5 vary across different groups, indicating variability in these variables among the groups.
V2 and V3 tend to have relatively low standard deviations across groups, suggesting consistency in their measurements.
V4 and V5 show slightly higher standard deviations, implying more variability in their measurements across groups.


*V6-V7:*
Standard deviations for V6 and V7 are consistently close to zero across groups, suggesting low variability in these variables and little influence on group differences.


*V8:*
Standard deviation values for V8 vary across groups, indicating variability in this variable among the groups.
Some groups, such as EXC and MIT, have relatively higher standard deviations for V8, suggesting more variability in their measurements compared to other groups

**Within group variance**

```{r}
calcWithinGroupsVariance <- function(variable,groupvariable)
  {
     # find out how many values the group variable can take
     groupvariable2 <- as.factor(groupvariable[[1]])
     levels <- levels(groupvariable2)
     numlevels <- length(levels)
     # get the mean and standard deviation for each group:
     numtotal <- 0
     denomtotal <- 0
     for (i in 1:numlevels)
     {
        leveli <- levels[i]
        levelidata <- variable[groupvariable==leveli,]
        levelilength <- length(levelidata)
        # get the standard deviation for group i:
        sdi <- sd(levelidata)
        numi <- (levelilength - 1)*(sdi * sdi)
        denomi <- levelilength
        numtotal <- numtotal + numi
        denomtotal <- denomtotal + denomi
     }
     # calculate the within-groups variance
     Vw <- numtotal / (denomtotal - numlevels)
     return(Vw)
}

calcWithinGroupsVariance(yeast[2],yeast[10])
```

The value we got, **0.01174351**, appears to be the within-group variance. Within-group variance measures the variability or spread of data within each group or category in a dataset. 



```{r}
calcBetweenGroupsVariance <- function(variable,groupvariable)
  {
     # find out how many values the group variable can take
     groupvariable2 <- as.factor(groupvariable[[1]])
     levels <- levels(groupvariable2)
     numlevels <- length(levels)
     # calculate the overall grand mean:
     grandmean <- as.numeric(sapply(variable, mean, na.rm = TRUE))
     # get the mean and standard deviation for each group:
     numtotal <- 0
     denomtotal <- 0
     for (i in 1:numlevels)
     {
        leveli <- levels[i]
        levelidata <- variable[groupvariable==leveli,]
        levelilength <- length(levelidata)
        # get the mean and standard deviation for group i:
        meani <- mean(levelidata)
        sdi <- sd(levelidata)
        numi <- levelilength * ((meani - grandmean)^2)
        denomi <- levelilength
        numtotal <- numtotal + numi
        denomtotal <- denomtotal + denomi
     }
     # calculate the between-groups variance
     Vb <- numtotal / (numlevels - 1)
     Vb <- Vb[[1]]
     return(Vb)
}

calcBetweenGroupsVariance(yeast[2],yeast[10])
```
The value we got, **1.1829161**, appears to be the between-group variance.



```{r}
calcSeparations <- function(variables,groupvariable)
  {
     # find out how many variables we have
     variables <- as.data.frame(variables)
     numvariables <- length(variables)
     # find the variable names
     variablenames <- colnames(variables)
     # calculate the separation for each variable
     for (i in 1:numvariables)
     {
        variablei <- variables[i]
        variablename <- variablenames[i]
        Vw <- calcWithinGroupsVariance(variablei, groupvariable)
        Vb <- calcBetweenGroupsVariance(variablei, groupvariable)
        sep <- Vb/Vw
        print(paste("variable",variablename,"Vw=",Vw,"Vb=",Vb,"separation=",sep))
     }
}

```

```{r}

calcSeparations(yeast[2:9], yeast[10])
```
Here for each variables, we can get Variance within the group, and Variance between the group and separation.
**Observation from the separations**

*Variability in Vw and Vb:* There is significant variability in the values of Vw and Vb across the different variables (V2 through V9). For instance, Vw ranges from as low as 0.0015 to as high as 0.0144, and Vb ranges from 0.0159 to 1.1829. This suggests that different variables may have different characteristics or behaviors.


Relationship between Vw, Vb, and Separation: There seems to be a relationship between Vw, Vb, and the separation value. While the exact nature of this relationship would require further analysis, it appears that changes in Vw and Vb are associated with changes in separation. This could indicate some form of dependency or correlation between these variables.


*Differences in Separation:*
The separation values (ranging from around 5 to 139) indicate differences in the degree of separation between Vw and Vb for each variable. Variables with higher separation values have more distinct values for Vw and Vb compared to those with lower separation values.

Potential Patterns or Trends: Further analysis could reveal potential patterns or trends in the data. For example, it may be interesting to investigate if there is any relationship between the magnitude of Vw/Vb and the separation value across variables. Additionally, clustering analysis or visualization techniques could help identify any inherent groupings or similarities among the variables.



**Between-groups Covariance and Within-groups Covariance for Two Variables**
```{r}
calcWithinGroupsCovariance <- function(variable1,variable2,groupvariable)
{
  # find out how many values the group variable can take
  groupvariable2 <- as.factor(groupvariable[[1]])
  levels <- levels(groupvariable2)
  numlevels <- length(levels)
  # get the covariance of variable 1 and variable 2 for each group:
  Covw <- 0
  for (i in 1:numlevels)
  {
    leveli <- levels[i]
    levelidata1 <- variable1[groupvariable==leveli,]
    levelidata2 <- variable2[groupvariable==leveli,]
    mean1 <- mean(levelidata1)
    mean2 <- mean(levelidata2)
    levelilength <- length(levelidata1)
    # get the covariance for this group:
    term1 <- 0
    for (j in 1:levelilength)
    {
      term1 <- term1 + ((levelidata1[j] - mean1)*(levelidata2[j] - mean2))
    }
    Cov_groupi <- term1 # covariance for this group
    Covw <- Covw + Cov_groupi
  }
  totallength <- nrow(variable1)
  Covw <- Covw / (totallength - numlevels)
  return(Covw)
}

```

```{r}
calcWithinGroupsCovariance(yeast[5],yeast[8],yeast[10])
```
the value we got , *-0.0008503556* is the covariance with in the group


Next, we are going to use multigroup library  to get calculate varience between and within groups



```{r}
library(multigroup)

result <- TBWvariance(yeast[,2:9], yeast[,10])

# Extract the desired outputs
within_var <- result$Within.Var
between_var <- result$Between.Var

# Print the extracted outputs
print(data.frame(within_var))


```

here we can see the Dataframe which contains varience within group 

```{r}
print(data.frame(between_var))
```

here we can see the Dataframe which contains varience between groups


**Calculating Correlations for Multivariate Data**


```{r}
cor.test(yeast$V2, yeast$V3)

```

*Correlation Coefficient (r):*

The correlation coefficient (r) between V2 and V3 is approximately 0.5816.
This indicates a moderately strong positive linear relationship between V2 and V3.


*Test Statistic:*
The test statistic (t) is approximately 27.526.
Degrees of freedom (df) is 1482.


*Significance:*
The p-value is extremely small (less than 2.2e-16), suggesting strong evidence against the null hypothesis.
Therefore, we reject the null hypothesis, indicating that there is a significant correlation between V2 and V3 in the yeast dataset.


*Confidence Interval:*

The 95% confidence interval for the correlation coefficient ranges from approximately 0.5469 to 0.6143.
This interval indicates that we are 95% confident that the true correlation between V2 and V3 falls within this range.



**Standardising Variables**

```{r}
standardisedconcentrations <- as.data.frame(scale(yeast[2:9]))
```

```{r}
sapply(standardisedconcentrations,mean)
```

```{r}
sapply(standardisedconcentrations,sd)
```

here we see we standarised all the variables.



**Principal Component Analysis**

we are going to perform Principal Component Analysis. we are using 8 variables. using PCA we can reduce the dimensionality of data while retaining the majority of variation. 

```{r}
yeast.pca <- prcomp(standardisedconcentrations) 
```

```{r}
yeast.pca
```

**Observations from the above data**

*Standard Deviations:*

The standard deviations for the original variables (V2 through V9) range from approximately 0.6376 to 1.3469.
These values indicate the variability or spread of the data along each principal component (PC) axis.


*Rotation Matrix:*

The rotation matrix represents the loadings of each original variable on the principal components.
Each row corresponds to an original variable (V2 through V9), and each column represents a principal component (PC1 through PC8).
Larger absolute values in the rotation matrix indicate stronger contributions of the corresponding original variable to the principal component.


*Interpretation:*

PC1: V2, V3, V5, and V8 have relatively higher loadings, suggesting that they contribute more to the variance captured by PC1.
PC2: V4, V5, V6, and V9 have higher loadings, indicating their importance in capturing variance along PC2.
PC3: V7 has the highest loading, suggesting it contributes significantly to the variance along PC3.
PC4 to PC8: Each subsequent PC captures decreasing amounts of variance, with varying contributions from different variables.


*Explained Variance:*

The variance explained by each principal component can be obtained from the squared standard deviations.
PC1 likely explains the most variance since it has the largest standard deviation, followed by PC2, and so on.
Cumulatively, the explained variance of all principal components can be calculated to assess how much of the total variance in the data is captured.



**The summary of PCA is given below**
```{r}
summary(yeast.pca)
```


```{r}
yeast.pca$sdev
```
**interpreting these metrics, we can derive the following insights:**


1. PC1 explains the largest proportion of variance (22.68%) and has the highest standard deviation (1.3469).

2.PC2 also captures a substantial amount of variance (15.88%), followed by PC3 (12.77%) and PC4 (12.43%).

3.Together, the first four principal components (PC1 to PC4) account for over 63% of the total variance in the data.

4.As we add more components, the cumulative proportion of explained variance increases, reaching 100% when all eight components are included.

Based on this information, a common approach might be to retain the first few principal components that capture a significant portion of the total variance while discarding those that contribute less. In this case, PC1 to PC4 seem particularly important, as they collectively explain over 63% of the variance, making them potentially valuable for data reduction or analysis



```{r}
screeplot(yeast.pca, type="lines")
```
the above figure shows the amount of variance explained by Principal Components where PCA is applied to all variables.

The screen plot of PCA which is given above represents
first 6 Principal components explains around 85% of variation in data which would be enough for the Classification and Statistical Analysis. 

first 4 Principal components explains around 63% of variation in data. we can also take 4 components which would also be enough for further calculation. it preservers around 63% of variance



```{r}
(yeast.pca$sdev)^2
```



```{r}
sum((yeast.pca$sdev)^2)
```
 This contains a matrix with the loadings of each principal component, where the first column in the matrix contains the loadings for the first principal component, the second column contains the loadings for the second principal component, and so on.
 
```{r}
yeast.pca$rotation[,1]
```
Another way to approach the computation of the first principal component is by creating a custom function that computes it based on the loadings and the values of the input variables

```{r}
calcpc <- function(variables,loadings)
  {
     # find the number of samples in the data set
     as.data.frame(variables)
     numsamples <- nrow(variables)
     # make a vector to store the component
     pc <- numeric(numsamples)
     # find the number of variables
     numvariables <- length(variables)
     # calculate the value of the component for each sample
     for (i in 1:numsamples)
     {
        valuei <- 0
        for (j in 1:numvariables)
        {
           valueij <- variables[i,j]
           loadingj <- loadings[j]
           valuei <- valuei + (valueij * loadingj)
        }
        pc[i] <- valuei
     }
     return(pc)
  }
```

We can then use the function to calculate the values of the first principal component for each sample in our yeast data:
```{r}
head(calcpc(standardisedconcentrations, yeast.pca$rotation[,1]))

```

the values of the first principal component are stored in the variable yeast.pca$x[,1] that was returned by the “prcomp()” function, so we can compare those values to the ones that we calculated
```{r}
head(yeast.pca$x[,1])
```
Similarly, we can obtain the loadings for the second principal component

```{r}
yeast.pca$rotation[,2]
```
**Scatterplots of the Principal Components**

we can plot the principal conmponent of principal components like this


```{r}
library(ggfortify)
df <- yeast[2:9]
pca_res <- prcomp(df, scale. = TRUE)

autoplot(pca_res)

```


```{r}

library(rgl)

scores = as.data.frame(yeast.pca$x)

plot3d(scores[,1:3], 
       size=5,
       col = seq(nrow(scores)))
 
text3d(scores[,1:3],
       texts=c(rownames(scores)), 
       cex= 0.7, pos=3)

```


3d plot is not visible here. we can use R to see 3 d graph between PCA1, PCA2 and PCA3.


**Conclusion of PCA analysis:**


*Significant Variance Explained:* The first four principal components (PC1 to PC4) collectively explain over 63% of the total variance in the data. This indicates that these components capture the most significant patterns and variability within the dataset.


*Incremental Variance Explanation:* As more principal components are considered, the cumulative proportion of explained variance increases, reaching 100% when all eight components are included. However, the additional variance explained by including more components diminishes compared to the initial ones.


*Selection of Components:* A common approach in PCA is to retain principal components that capture a substantial portion of the total variance while discarding less informative components. In this case, selecting the first four principal components seems reasonable, as they capture a significant amount of variance.


*Scree Plot Analysis:* The scree plot visually confirms the findings from the PCA results, showing that the first six principal components explain around 85% of the variation in the data. This suggests that these six components might be sufficient for analysis purposes.
































